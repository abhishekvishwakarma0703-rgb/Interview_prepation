# Rate Limiter - Complete System Design Guide

**A Comprehensive Reference for Understanding and Designing Rate Limiting Systems**

-----

## Table of Contents

1. [Introduction & Motivation](#1-introduction--motivation)
1. [The Core Problem](#2-the-core-problem)
1. [Requirements & Scope](#3-requirements--scope)
1. [Rate Limiting Algorithms (Deep Dive)](#4-rate-limiting-algorithms)
- Fixed Window Counter
- Sliding Window Log
- Sliding Window Counter
- Token Bucket
- Leaky Bucket
1. [Distributed Rate Limiting](#5-distributed-rate-limiting)
1. [System Design & Architecture](#6-system-design--architecture)
1. [Advanced Topics](#7-advanced-topics)
1. [Real-World Case Studies](#8-real-world-case-studies)
1. [Interview Framework](#9-interview-framework)

-----

## 1. Introduction & Motivation

### 1.1 What is Rate Limiting?

**Rate Limiting** is a technique to control the rate at which users or services can access a resource or perform an action.

```
Without Rate Limiting:
User sends 1,000,000 requests in 1 second
         â†“
    All hit your API
         â†“
    Server overloaded â†’ Crashes ğŸ’¥

With Rate Limiting:
User sends 1,000,000 requests in 1 second
         â†“
    First 100 allowed â†’ rest rejected
         â†“
    Server stays healthy âœ“
    User gets: "429 Too Many Requests"
```

### 1.2 Why Do We Need Rate Limiting?

**Problem 1: Resource Protection**

```
Scenario: Your API server can handle 10,000 QPS maximum

Without rate limiting:
â”œâ”€ One user sends 50,000 requests/second
â”œâ”€ Server CPU/memory maxed out
â”œâ”€ ALL users experience slow response
â””â”€ System becomes unavailable

With rate limiting:
â”œâ”€ Limit each user to 100 requests/second
â”œâ”€ Abusive user gets throttled
â”œâ”€ Other users continue to work normally
â””â”€ System remains stable
```

**Problem 2: Preventing Abuse & Attacks**

```
DDoS Attack Scenario:
â”œâ”€ Attacker controls 10,000 bots
â”œâ”€ Each bot sends 100 requests/second
â”œâ”€ Total: 1,000,000 requests/second
â””â”€ Your servers: ğŸ’€

Rate Limiting Defense:
â”œâ”€ Limit per IP: 10 requests/second
â”œâ”€ 10,000 IPs Ã— 10 req/sec = 100,000 req/sec
â”œâ”€ Still high, but manageable
â””â”€ Combined with other defenses (IP blocking, CAPTCHA)
```

**Problem 3: Cost Control**

```
Third-party API costs:
â”œâ”€ Google Maps API: $5 per 1,000 requests
â”œâ”€ Without rate limiting: User bug causes 10M requests
â”œâ”€ Cost: $50,000 in one day! ğŸ’¸
â””â”€ With rate limiting: Capped at $500/day âœ“

Database query costs:
â”œâ”€ Complex query costs 100ms CPU time
â”œâ”€ Infinite loop causes 10,000 queries
â”œâ”€ Database overloaded
â””â”€ Rate limit prevents runaway queries
```

**Problem 4: Fair Resource Distribution**

```
Shared SaaS Platform:
â”œâ”€ 1000 customers share infrastructure
â”œâ”€ Customer A: 1,000 req/sec
â”œâ”€ Customer B: 100,000 req/sec (monopolizes resources)
â””â”€ Customer A gets slow service (unfair!)

With rate limiting per customer:
â”œâ”€ Each customer: 1,000 req/sec maximum
â”œâ”€ Fair usage across all customers
â””â”€ Everyone gets predictable performance
```

### 1.3 Real-World Examples

|Service        |Rate Limit             |Why                                      |
|---------------|-----------------------|-----------------------------------------|
|**Twitter API**|300 requests / 15 min  |Prevent scraping, ensure fair access     |
|**GitHub API** |5,000 requests / hour  |Control server load, prevent abuse       |
|**Stripe API** |100 requests / second  |Protect payment processing infrastructure|
|**Google Maps**|$200 free credit/month |Cost control, prevent bill shock         |
|**OpenAI API** |Tier-based (tokens/min)|GPU resource management                  |

-----

## 2. The Core Problem

### 2.1 The Challenge

**Question:** How do you enforce â€œ100 requests per minuteâ€ for millions of users?

**Challenges:**

```
Challenge 1: Counting Requests
â”œâ”€ How do you count requests accurately?
â”œâ”€ Where do you store the count?
â”œâ”€ How do you handle distributed servers?
â””â”€ What happens when counter resets?

Challenge 2: Performance
â”œâ”€ Rate limit check must be FAST (< 1ms)
â”œâ”€ Cannot slow down every request
â”œâ”€ Must scale to millions of users
â””â”€ Must handle high concurrent load

Challenge 3: Accuracy
â”œâ”€ Must be fair (no user gets unfair advantage)
â”œâ”€ Must prevent gaming the system
â”œâ”€ Must handle edge cases (burst traffic)
â””â”€ Must be consistent across multiple servers

Challenge 4: Resource Efficiency
â”œâ”€ Cannot use too much memory
â”œâ”€ Cannot use too much CPU
â”œâ”€ Cannot use too much network bandwidth
â””â”€ Must clean up old data
```

### 2.2 Simple (Naive) Approach - Why It Fails

**Attempt 1: In-Memory Counter**

```python
# Store in application memory
user_requests = {}  # user_id -> count

def is_allowed(user_id):
    if user_id not in user_requests:
        user_requests[user_id] = 0
    
    user_requests[user_id] += 1
    
    if user_requests[user_id] <= 100:
        return True
    return False
```

**Problems:**

```
âŒ When does the counter reset? Never!
âŒ What if you have multiple servers?
   â”œâ”€ Server 1 has count = 50
   â”œâ”€ Server 2 has count = 50
   â””â”€ User made 100 requests total, but both allow more!
âŒ Memory leak - user_requests grows forever
âŒ Lost on server restart
```

**Attempt 2: Add Time Window**

```python
import time

user_requests = {}  # user_id -> {'count': 0, 'window_start': timestamp}

def is_allowed(user_id):
    now = time.time()
    
    if user_id not in user_requests:
        user_requests[user_id] = {'count': 1, 'window_start': now}
        return True
    
    # Check if window expired (60 seconds)
    if now - user_requests[user_id]['window_start'] >= 60:
        # Reset window
        user_requests[user_id] = {'count': 1, 'window_start': now}
        return True
    
    # Within window
    user_requests[user_id]['count'] += 1
    
    if user_requests[user_id]['count'] <= 100:
        return True
    return False
```

**Better, but still problems:**

```
âŒ Still doesn't work with multiple servers
âŒ Edge case: Burst at window boundary
   â”œâ”€ 00:00:59 - User sends 100 requests (allowed)
   â”œâ”€ 00:01:00 - Window resets
   â”œâ”€ 00:01:01 - User sends 100 requests (allowed)
   â””â”€ Result: 200 requests in 2 seconds! (2x the limit)
âŒ Memory still grows unbounded
```

**This is why we need sophisticated algorithms!**

-----

## 3. Requirements & Scope

### 3.1 Functional Requirements

```
Core Operations:
â”œâ”€ allow_request(user_id) â†’ boolean
â”œâ”€ Configure rate limit rules (100/min, 1000/hour, etc.)
â”œâ”€ Support multiple rate limit dimensions:
â”‚  â”œâ”€ Per user
â”‚  â”œâ”€ Per IP address
â”‚  â”œâ”€ Per API key
â”‚  â””â”€ Per endpoint
â””â”€ Return meaningful error messages

Advanced (optional):
â”œâ”€ Different limits for different user tiers (free vs premium)
â”œâ”€ Dynamic rate limits based on server load
â”œâ”€ Temporary rate limit increases
â””â”€ Rate limit analytics/monitoring
```

### 3.2 Non-Functional Requirements

|Requirement          |Target              |Why                                               |
|---------------------|--------------------|--------------------------------------------------|
|**Latency**          |< 1ms (p99)         |Cannot slow down API requests                     |
|**Accuracy**         |99.9%+              |Fair enforcement, prevent gaming                  |
|**Scalability**      |Handle 10M users    |Large-scale systems                               |
|**Availability**     |99.99%              |Rate limiter failure shouldnâ€™t break entire system|
|**Memory Efficiency**|O(users) space      |Cannot store unlimited data                       |
|**Fault Tolerance**  |Graceful degradation|Fail open or fail closed?                         |

### 3.3 Failure Modes

**Question: When rate limiter fails, what should happen?**

```
Option A: Fail Open (Allow all requests)
â”œâ”€ Pro: Service remains available
â”œâ”€ Con: No protection during outage
â””â”€ Use when: Availability > Security (e.g., social media)

Option B: Fail Closed (Reject all requests)
â”œâ”€ Pro: Protection maintained
â”œâ”€ Con: Service unavailable
â””â”€ Use when: Security > Availability (e.g., banking)

Option C: Hybrid
â”œâ”€ Allow requests with high priority/authentication
â”œâ”€ Reject anonymous/low-priority requests
â””â”€ Use when: Need balance (e.g., e-commerce)
```

-----

## 4. Rate Limiting Algorithms

### 4.1 Fixed Window Counter

**Concept:** Divide time into fixed windows (e.g., 1 minute). Count requests in each window.

**How It Works:**

```
Time windows (60 seconds each):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Window 1     â”‚ Window 2     â”‚ Window 3     â”‚
â”‚ 00:00 - 01:00â”‚ 01:00 - 02:00â”‚ 02:00 - 03:00â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Limit: 100 requests per minute

Window 1 (00:00 - 01:00):
â”œâ”€ Request 1 at 00:00:10 â†’ count = 1 â†’ Allow âœ“
â”œâ”€ Request 2 at 00:00:20 â†’ count = 2 â†’ Allow âœ“
â”œâ”€ ...
â”œâ”€ Request 100 at 00:00:50 â†’ count = 100 â†’ Allow âœ“
â””â”€ Request 101 at 00:00:55 â†’ count = 101 â†’ Reject âœ—

Window 2 (01:00 - 02:00):
â””â”€ Counter resets to 0 at exactly 01:00:00
```

**Implementation:**

```python
import time
import math

class FixedWindowCounter:
    def __init__(self, max_requests, window_size_seconds):
        self.max_requests = max_requests
        self.window_size = window_size_seconds
        self.counters = {}  # user_id -> {'count': int, 'window_start': timestamp}
    
    def allow_request(self, user_id):
        now = time.time()
        
        # Calculate current window start
        current_window = math.floor(now / self.window_size) * self.window_size
        
        if user_id not in self.counters:
            self.counters[user_id] = {
                'count': 1,
                'window_start': current_window
            }
            return True
        
        user_data = self.counters[user_id]
        
        # Check if we're in a new window
        if user_data['window_start'] < current_window:
            # Reset for new window
            self.counters[user_id] = {
                'count': 1,
                'window_start': current_window
            }
            return True
        
        # Same window - increment counter
        if user_data['count'] < self.max_requests:
            user_data['count'] += 1
            return True
        
        return False  # Exceeded limit

# Usage
limiter = FixedWindowCounter(max_requests=100, window_size_seconds=60)

# Simulate requests
for i in range(150):
    user_id = "user_123"
    allowed = limiter.allow_request(user_id)
    print(f"Request {i+1}: {'Allowed' if allowed else 'Rejected'}")
```

**Visual Example:**

```
Limit: 5 requests per minute

Timeline:
â”œâ”€ 00:00:10 - Request 1 â†’ count = 1 â†’ âœ“ Allowed
â”œâ”€ 00:00:20 - Request 2 â†’ count = 2 â†’ âœ“ Allowed
â”œâ”€ 00:00:30 - Request 3 â†’ count = 3 â†’ âœ“ Allowed
â”œâ”€ 00:00:40 - Request 4 â†’ count = 4 â†’ âœ“ Allowed
â”œâ”€ 00:00:50 - Request 5 â†’ count = 5 â†’ âœ“ Allowed
â”œâ”€ 00:00:55 - Request 6 â†’ count = 6 â†’ âœ— Rejected (limit reached)
â”œâ”€ 01:00:00 - Window resets â†’ count = 0
â””â”€ 01:00:05 - Request 7 â†’ count = 1 â†’ âœ“ Allowed
```

**The Boundary Problem (Critical!):**

```
Limit: 100 requests per minute

Timeline:
â”œâ”€ 00:00:00 - Window 1 starts
â”œâ”€ 00:00:40 - User sends 50 requests â†’ Allowed âœ“
â”œâ”€ 00:00:59 - User sends 50 requests â†’ Allowed âœ“ (total: 100)
â”‚   [End of Window 1: 100 requests in last 60 seconds]
â”‚
â”œâ”€ 01:00:00 - Window 2 starts, counter resets to 0
â”œâ”€ 01:00:01 - User sends 50 requests â†’ Allowed âœ“
â”œâ”€ 01:00:20 - User sends 50 requests â†’ Allowed âœ“ (total: 100)
â”‚
â””â”€ Analysis:
    From 00:00:40 to 01:00:20 (40 seconds):
    â””â”€ User sent 200 requests!
    â””â”€ That's 300 requests per minute rate!
    â””â”€ DOUBLE the intended limit! âœ—
```

**Visualization of the Problem:**

```
Window 1: 00:00 - 01:00          Window 2: 01:00 - 02:00
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ................ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚   â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ ................â”‚
â”‚                  50 req    â”‚   â”‚50 req                      â”‚
â”‚                  at :40-:59â”‚   â”‚at :00-:20                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                        40 seconds: 200 requests!
                        (Should be max 67 for 40 seconds)
```

**Pros:**

```
âœ“ Simple to implement
âœ“ Memory efficient: O(1) per user
âœ“ Fast: O(1) lookup and update
âœ“ Easy to understand
```

**Cons:**

```
âœ— Burst traffic at window boundaries (can exceed limit 2x)
âœ— Hard reset creates usage spikes
âœ— Not fair across time
```

**When to Use:**

```
âœ“ Simple use cases with relaxed accuracy
âœ“ Internal services (not customer-facing)
âœ“ When implementation simplicity matters most
âœ— Don't use when strict rate limiting needed
```

-----

### 4.2 Sliding Window Log

**Concept:** Keep a log (list) of timestamps for each request. Count requests in the last N seconds.

**How It Works:**

```
Limit: 5 requests per minute (60 seconds)

User's request log (stored as timestamps):
[00:00:10, 00:00:25, 00:00:40, 00:01:05, 00:01:20]

At time 00:01:30, new request arrives:
1. Remove timestamps older than 60 seconds
   â””â”€ Current time: 00:01:30 (90 seconds)
   â””â”€ Remove: 00:00:10 (80 seconds ago - too old)
   â””â”€ Remove: 00:00:25 (65 seconds ago - too old)
   â””â”€ Keep: 00:00:40, 00:01:05, 00:01:20

2. Count remaining timestamps: 3 requests

3. Check: 3 < 5? Yes â†’ Allow request âœ“

4. Add current timestamp to log:
   [00:00:40, 00:01:05, 00:01:20, 00:01:30]
```

**Implementation:**

```python
import time
from collections import deque

class SlidingWindowLog:
    def __init__(self, max_requests, window_size_seconds):
        self.max_requests = max_requests
        self.window_size = window_size_seconds
        # user_id -> deque of timestamps
        self.logs = {}
    
    def allow_request(self, user_id):
        now = time.time()
        
        if user_id not in self.logs:
            self.logs[user_id] = deque()
        
        user_log = self.logs[user_id]
        
        # Remove timestamps outside the window
        cutoff_time = now - self.window_size
        while user_log and user_log[0] <= cutoff_time:
            user_log.popleft()
        
        # Check if we can allow this request
        if len(user_log) < self.max_requests:
            user_log.append(now)
            return True
        
        return False
    
    def cleanup(self):
        """Optional: Remove empty user logs to save memory"""
        now = time.time()
        cutoff = now - self.window_size
        
        to_delete = []
        for user_id, log in self.logs.items():
            # Remove old entries
            while log and log[0] <= cutoff:
                log.popleft()
            
            # Mark empty logs for deletion
            if not log:
                to_delete.append(user_id)
        
        for user_id in to_delete:
            del self.logs[user_id]

# Usage
limiter = SlidingWindowLog(max_requests=5, window_size_seconds=60)

# Test
print(limiter.allow_request("user_123"))  # True
time.sleep(0.1)
print(limiter.allow_request("user_123"))  # True
# ... 3 more requests ...
print(limiter.allow_request("user_123"))  # True (5th)
print(limiter.allow_request("user_123"))  # False (exceeds limit)
```

**Visual Example:**

```
Limit: 5 requests per 60 seconds

Timeline with sliding window:

Time: 00:00:00
Log: []
Request â†’ count = 0 < 5 â†’ âœ“ Allowed
Log: [00:00:00]

Time: 00:00:15
Log: [00:00:00] (1 request in last 60s)
Request â†’ count = 1 < 5 â†’ âœ“ Allowed
Log: [00:00:00, 00:00:15]

Time: 00:00:30
Log: [00:00:00, 00:00:15] (2 requests in last 60s)
Request â†’ count = 2 < 5 â†’ âœ“ Allowed
Log: [00:00:00, 00:00:15, 00:00:30]

Time: 01:00:10
Window: [00:00:10 - 01:00:10]
Old log: [00:00:00, 00:00:15, 00:00:30]
After cleanup: [00:00:15, 00:00:30] (00:00:00 is older than 60s)
Request â†’ count = 2 < 5 â†’ âœ“ Allowed
Log: [00:00:15, 00:00:30, 01:00:10]
```

**No Boundary Problem!**

```
Limit: 100 requests per minute

Timeline:
00:00:40 - User sends 50 requests
         - Log has 50 timestamps

00:00:59 - User sends 50 requests  
         - Log has 100 timestamps
         - Next request rejected âœ—

01:00:01 - User tries to send more
         - Window: [00:00:01 - 01:00:01]
         - Cleanup: Remove timestamps before 00:00:01
         - Only timestamps from 00:00:40 onwards remain
         - Log still has ~100 requests
         - Request rejected âœ—

01:00:40 - User tries again
         - Window: [00:00:40 - 01:00:40]
         - Cleanup: Remove timestamps before 00:00:40
         - First 50 requests (from 00:00:40) now removed
         - Only 50 timestamps remain (from 00:00:59)
         - Request allowed âœ“

Result: Smooth, accurate rate limiting!
```

**Memory Analysis:**

```
Fixed Window: O(1) per user
â””â”€ Only store: count + window_start

Sliding Window Log: O(limit) per user
â””â”€ Store: All timestamps in window
â””â”€ Example: 100 req/min â†’ 100 timestamps per user
â””â”€ 1M users Ã— 100 timestamps Ã— 8 bytes = 800 MB

For high-rate limits, this can be expensive!
```

**Pros:**

```
âœ“ Very accurate - no boundary problem
âœ“ True sliding window behavior
âœ“ Fair enforcement across time
```

**Cons:**

```
âœ— Memory intensive: O(limit Ã— users)
âœ— Cleanup overhead (removing old timestamps)
âœ— Can be slow with very high rate limits
```

**When to Use:**

```
âœ“ When accuracy is critical
âœ“ Low to medium rate limits (< 1000/window)
âœ“ Security-sensitive applications
âœ— Don't use for very high rate limits (memory cost)
```

-----

### 4.3 Sliding Window Counter (Hybrid)

**Concept:** Combine fixed window efficiency with sliding window accuracy. Use weighted count from two adjacent windows.

**How It Works:**

```
Current time: 00:01:15 (75 seconds = 1 minute 15 seconds)
Window size: 60 seconds
Limit: 100 requests per minute

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Previous Window       â”‚    Current Window       â”‚
â”‚   00:00 - 01:00        â”‚    01:00 - 02:00       â”‚
â”‚   Count: 80 requests    â”‚    Count: 30 requests   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†‘
                    Current time: 01:15
                    (15 seconds into current window)

Calculate estimated count for last 60 seconds:

Time range we care about: [00:01:15 - 01:01:15]

Overlap with previous window: 45 seconds (00:01:15 - 01:00:00)
Overlap with current window: 15 seconds (01:00:00 - 01:01:15)

Formula:
Estimated count = (Previous window count Ã— overlap% with previous)
                + (Current window count)

Estimated count = (80 Ã— 45/60) + 30
                = (80 Ã— 0.75) + 30
                = 60 + 30
                = 90 requests

90 < 100? Yes â†’ Allow request âœ“
```

**Detailed Visual:**

```
Timeline:
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                Previous Window         â”‚   Current Window   â”‚
â”‚                00:00 - 01:00          â”‚   01:00 - 02:00   â”‚
â”‚                (80 requests)           â”‚   (30 requests)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                          â†‘
                                    Current: 01:15

Sliding window we want to check: [00:01:15 - 01:01:15]
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Part in prev window â”‚ Part in curr  â”‚
â”‚   (45 seconds)      â”‚   (15 sec)    â”‚
â”‚   75% of window     â”‚   25% of win  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

Weight calculation:
â”œâ”€ Previous window contribution: 80 Ã— 0.75 = 60
â”œâ”€ Current window contribution: 30 Ã— 1.0 = 30
â””â”€ Total estimate: 90 requests
```

**Implementation:**

```python
import time
import math

class SlidingWindowCounter:
    def __init__(self, max_requests, window_size_seconds):
        self.max_requests = max_requests
        self.window_size = window_size_seconds
        # user_id -> {'prev_count': int, 'prev_window': timestamp,
        #             'curr_count': int, 'curr_window': timestamp}
        self.windows = {}
    
    def allow_request(self, user_id):
        now = time.time()
        
        # Calculate current window start
        current_window = math.floor(now / self.window_size) * self.window_size
        
        if user_id not in self.windows:
            self.windows[user_id] = {
                'prev_count': 0,
                'prev_window': current_window - self.window_size,
                'curr_count': 0,
                'curr_window': current_window
            }
        
        user_data = self.windows[user_id]
        
        # Check if we've moved to a new window
        if user_data['curr_window'] < current_window:
            # Shift windows: current becomes previous
            user_data['prev_count'] = user_data['curr_count']
            user_data['prev_window'] = user_data['curr_window']
            user_data['curr_count'] = 0
            user_data['curr_window'] = current_window
        
        # Calculate weighted count
        elapsed_in_current = now - user_data['curr_window']
        previous_window_weight = 1 - (elapsed_in_current / self.window_size)
        
        estimated_count = (
            user_data['prev_count'] * previous_window_weight +
            user_data['curr_count']
        )
        
        # Check if request allowed
        if estimated_count < self.max_requests:
            user_data['curr_count'] += 1
            return True
        
        return False

# Usage
limiter = SlidingWindowCounter(max_requests=100, window_size_seconds=60)

allowed = limiter.allow_request("user_123")
print(f"Request allowed: {allowed}")
```

**Example Walkthrough:**

```
Limit: 10 requests per minute

Scenario:
â”œâ”€ 00:00:00 - 00:01:00: User makes 8 requests
â”œâ”€ 00:01:00 - 00:02:00: User makes 4 requests so far
â””â”€ Current time: 00:01:30 (30 seconds into second window)

Previous window: [00:00-00:01] = 8 requests
Current window: [00:01-00:02] = 4 requests
Time in current window: 30 seconds (50% of window)

Calculation:
â”œâ”€ Previous window weight: 1 - (30/60) = 0.5
â”œâ”€ Estimated count: (8 Ã— 0.5) + 4 = 4 + 4 = 8
â””â”€ 8 < 10 â†’ Allow âœ“

New request at 00:01:45 (45 seconds into window):
â”œâ”€ Previous window weight: 1 - (45/60) = 0.25
â”œâ”€ Current window now has 5 requests
â”œâ”€ Estimated count: (8 Ã— 0.25) + 5 = 2 + 5 = 7
â””â”€ 7 < 10 â†’ Allow âœ“
```

**Accuracy Comparison:**

```
Scenario: Burst at boundary
â”œâ”€ 00:00:50 - 00:01:00: 10 requests (in 10 seconds)
â”œâ”€ 00:01:00 - 00:01:10: 10 requests (in 10 seconds)
â””â”€ Total: 20 requests in 20 seconds (should be rejected!)

Fixed Window:
â”œâ”€ Window 1 [00:00-01:00]: 10 requests â†’ All allowed âœ“
â”œâ”€ Window 2 [01:00-02:00]: 10 requests â†’ All allowed âœ“
â””â”€ Result: 20 requests allowed âœ— (WRONG - allows 2x limit)

Sliding Window Log:
â”œâ”€ At 00:01:10, check last 60 seconds
â”œâ”€ Finds 20 requests in last 60 seconds
â””â”€ Result: Rejects requests after 10th âœ“ (CORRECT)

Sliding Window Counter:
â”œâ”€ At 00:01:05 (5 sec into window 2):
â”‚  â””â”€ Estimate: (10 Ã— 0.917) + 5 = 14.17
â”‚  â””â”€ Rejects 5th request in window 2 âœ“
â””â”€ Result: Approximately correct (better than fixed)
```

**Memory Usage:**

```
Fixed Window: O(1) per user
â”œâ”€ count + window_start

Sliding Window Log: O(limit) per user
â”œâ”€ All timestamps

Sliding Window Counter: O(1) per user
â”œâ”€ prev_count + curr_count + timestamps
â””â”€ Only 2 counters per user!

Winner: Same memory as Fixed Window! âœ“
```

**Pros:**

```
âœ“ Memory efficient: O(1) per user
âœ“ More accurate than fixed window
âœ“ Fast: O(1) lookup and update
âœ“ Smooth rate limiting (no hard resets)
```

**Cons:**

```
âœ— Not perfectly accurate (estimation)
âœ— Slightly more complex than fixed window
âœ— Small inaccuracies at window boundaries
```

**When to Use:**

```
âœ“ Best balance of accuracy and efficiency
âœ“ Most production systems use this!
âœ“ High-scale applications (Twitter, Stripe)
âœ“ When you need good accuracy without memory overhead
```

-----

### 4.4 Token Bucket

**Concept:** Imagine a bucket that holds tokens. Tokens are added at a constant rate. Each request consumes one token.

**Analogy:**

```
Think of it like a vending machine with coupons:
â”œâ”€ Bucket capacity: 100 coupons
â”œâ”€ Refill rate: 10 coupons per second
â”œâ”€ Each request costs 1 coupon
â””â”€ If bucket empty â†’ request denied

This allows bursts (use all 100 coupons quickly)
But sustained rate is limited (only 10/second long-term)
```

**How It Works:**

```
Bucket State:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bucket Capacity: 100    â”‚
â”‚ Current Tokens: 75      â”‚
â”‚ Refill Rate: 10/second  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Timeline:
â”œâ”€ 00:00:00 - Bucket has 75 tokens
â”œâ”€ 00:00:01 - Refill: +10 tokens â†’ 85 tokens
â”œâ”€ 00:00:02 - Request arrives, consume 1 token â†’ 84 tokens
â”œâ”€ 00:00:03 - Refill: +10 tokens â†’ 94 tokens
â”œâ”€ 00:00:04 - 5 requests arrive, consume 5 tokens â†’ 89 tokens
â”œâ”€ 00:00:05 - Refill: +10 tokens â†’ 99 tokens
â”œâ”€ 00:00:06 - Burst: 50 requests â†’ 49 tokens (all allowed!)
â”œâ”€ 00:00:07 - Refill: +10 tokens â†’ 59 tokens
â””â”€ 00:00:08 - Burst: 100 requests â†’ First 59 allowed, rest rejected
```

**Key Properties:**

```
1. Bucket Capacity (Burst Size):
   â””â”€ Maximum tokens bucket can hold
   â””â”€ Determines maximum burst

2. Refill Rate:
   â””â”€ Tokens added per second
   â””â”€ Determines sustained rate

Example:
â”œâ”€ Capacity: 100 tokens
â”œâ”€ Refill: 10 tokens/second
â”œâ”€ Burst: Can handle 100 requests immediately
â””â”€ Sustained: Max 10 requests/second over time
```

**Implementation:**

```python
import time

class TokenBucket:
    def __init__(self, capacity, refill_rate):
        """
        capacity: Maximum tokens in bucket (burst size)
        refill_rate: Tokens added per second
        """
        self.capacity = capacity
        self.refill_rate = refill_rate
        self.tokens = capacity  # Start with full bucket
        self.last_refill = time.time()
    
    def allow_request(self, tokens_needed=1):
        """
        Check if request can be allowed
        tokens_needed: Number of tokens required (default 1)
        """
        now = time.time()
        
        # Calculate tokens to add since last refill
        time_elapsed = now - self.last_refill
        tokens_to_add = time_elapsed * self.refill_rate
        
        # Refill bucket (but don't exceed capacity)
        self.tokens = min(self.capacity, self.tokens + tokens_to_add)
        self.last_refill = now
        
        # Check if we have enough tokens
        if self.tokens >= tokens_needed:
            self.tokens -= tokens_needed
            return True
        
        return False

# Usage
limiter = TokenBucket(capacity=100, refill_rate=10)

# Burst scenario
print("Burst test:")
for i in range(150):
    if limiter.allow_request():
        print(f"Request {i+1}: Allowed")
    else:
        print(f"Request {i+1}: Rejected")

# Sustained rate test
print("\nSustained rate test:")
for i in range(20):
    time.sleep(0.1)  # 100ms between requests
    if limiter.allow_request():
        print(f"Request {i+1}: Allowed")
    else:
        print(f"Request {i+1}: Rejected")
```

**Visual Representation:**

```
Token Bucket State Over Time:

Capacity: 10 tokens
Refill: 2 tokens/second

Time  Tokens  Event
â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0:00   10     Start (full bucket)
0:01   10     Refill +2, but capped at 10
0:02   10     Refill +2, but capped at 10
0:03   9      Request arrives (-1 token)
0:04   10     Refill +2, total 11 â†’ capped at 10
0:05   5      Burst: 5 requests (-5 tokens)
0:06   7      Refill +2 (+2 tokens)
0:07   9      Refill +2 (+2 tokens)
0:08   3      Burst: 6 requests (only 9 available)
              First 9 allowed, 7th rejected
0:09   5      Refill +2 (+2 tokens)
0:10   7      Refill +2 (+2 tokens)

Graph:
Tokens
  10 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   9 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆ
   8 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   7 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      â–ˆ â–ˆ
   6 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   5 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆ     â–ˆ
   4 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   3 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      â–ˆ
   2 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   1 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   0 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     0  2  4  6  8  10 (seconds)
```

**Comparison: Token Bucket vs Fixed Window**

```
Scenario: Limit should be 10 requests/second
User sends 15 requests at once

Fixed Window (1-second windows):
â”œâ”€ Window 1 [0:00-0:01]: First 10 allowed, 5 rejected
â”œâ”€ Window 2 [0:01-0:02]: Counter resets, 10 more allowed
â””â”€ Problem: Hard cutoff at window boundary

Token Bucket (capacity=10, refill=10/sec):
â”œâ”€ Initial: 10 tokens available
â”œâ”€ Burst: All 10 requests allowed immediately
â”œâ”€ 5 requests rejected (no tokens left)
â”œâ”€ After 0.5 seconds: +5 tokens refilled
â”œâ”€ 5 more requests can be allowed
â””â”€ Smooth: Allows burst, then gradual recovery
```

**Use Cases:**

```
Token Bucket is ideal for:

1. Network Traffic Shaping:
   â””â”€ Allow bursts, but limit sustained rate
   â””â”€ Example: ISP bandwidth limiting

2. API Rate Limiting with Bursts:
   â”œâ”€ Normal: 10 req/sec
   â”œâ”€ Allow occasional bursts up to 100 req
   â””â”€ Example: Stripe API

3. Resource Allocation:
   â””â”€ CPU time slices
   â””â”€ I/O bandwidth allocation

Real Example - Stripe API:
â”œâ”€ Bucket Capacity: 100 requests
â”œâ”€ Refill Rate: 1 request/second
â”œâ”€ Result: Can burst 100 requests immediately
â””â”€ Then limited to 1 req/sec sustained
```

**Pros:**

```
âœ“ Allows controlled bursts
âœ“ Smooth rate limiting
âœ“ Memory efficient: O(1) per user
âœ“ Fast: O(1) per request
âœ“ Flexible (can tune capacity vs rate separately)
```

**Cons:**

```
âœ— Slightly complex to understand
âœ— Requires floating-point arithmetic
âœ— Old tokens never expire (must handle edge cases)
```

**When to Use:**

```
âœ“ When bursts are acceptable (and even desirable)
âœ“ Network traffic shaping
âœ“ API rate limiting for flexibility
âœ“ When you want to be lenient to users
```

-----

### 4.5 Leaky Bucket

**Concept:** Imagine a bucket with a hole at the bottom. Requests are added to the bucket, and â€œleak outâ€ at a constant rate.

**Analogy:**

```
Think of it like a funnel:
â”œâ”€ Requests pour in at varying rates (top)
â”œâ”€ Requests leak out at constant rate (bottom hole)
â”œâ”€ If bucket overflows â†’ request rejected
â””â”€ Smooths out traffic spikes

Like a line at a theme park ride:
â”œâ”€ People arrive in bursts
â”œâ”€ Ride takes people at constant rate
â””â”€ If line too long â†’ people turned away
```

**How It Works:**

```
Bucket State:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bucket Capacity: 10 requests    â”‚
â”‚ Current Queue: 3 requests       â”‚
â”‚ Leak Rate: 2 requests/second    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Every second: Process 2 requests from queue

Timeline:
â”œâ”€ 00:00 - Queue: [R1, R2, R3]
â”œâ”€ 00:01 - Leak 2 requests (R1, R2 processed)
â”‚          Queue: [R3]
â”œâ”€ 00:02 - 5 new requests arrive
â”‚          Queue: [R3, R4, R5, R6, R7, R8]
â”œâ”€ 00:03 - Leak 2 requests (R3, R4 processed)
â”‚          Queue: [R5, R6, R7, R8]
â”œâ”€ 00:04 - 10 new requests arrive
â”‚          Queue size would be 14 (4 + 10)
â”‚          But capacity is 10!
â”‚          First 6 added, next 4 rejected âœ—
â””â”€ 00:05 - Leak 2 requests, queue: 8 remaining
```

**Key Difference from Token Bucket:**

```
Token Bucket:
â”œâ”€ Tokens accumulate over time
â”œâ”€ Can spend all tokens at once (burst)
â””â”€ Refill continuously

Leaky Bucket:
â”œâ”€ Requests accumulate in queue
â”œâ”€ Processed at constant rate (no bursts)
â””â”€ Leak continuously

Example:
â”œâ”€ 100 requests arrive instantly
â”‚
Token Bucket:
â””â”€ If 100 tokens available â†’ all processed immediately âœ“

Leaky Bucket:
â””â”€ All added to queue â†’ processed at fixed rate over time
    (e.g., 10/second â†’ takes 10 seconds to process all)
```

**Implementation (Queue-based):**

```python
import time
from collections import deque

class LeakyBucket:
    def __init__(self, capacity, leak_rate):
        """
        capacity: Maximum requests that can be queued
        leak_rate: Requests processed per second
        """
        self.capacity = capacity
        self.leak_rate = leak_rate
        self.queue = deque()
        self.last_leak = time.time()
    
    def _leak(self):
        """Process (leak) requests based on elapsed time"""
        now = time.time()
        elapsed = now - self.last_leak
        
        # Calculate how many requests to leak
        requests_to_leak = int(elapsed * self.leak_rate)
        
        if requests_to_leak > 0:
            # Remove leaked requests from queue
            for _ in range(min(requests_to_leak, len(self.queue))):
                self.queue.popleft()
            
            self.last_leak = now
    
    def allow_request(self):
        """Try to add request to bucket"""
        # First, leak existing requests
        self._leak()
        
        # Check if bucket has space
        if len(self.queue) < self.capacity:
            self.queue.append(time.time())
            return True
        
        return False  # Bucket full
    
    def get_queue_size(self):
        """Get current number of queued requests"""
        self._leak()
        return len(self.queue)

# Usage
limiter = LeakyBucket(capacity=10, leak_rate=2)

# Test burst
print("Burst of 15 requests:")
for i in range(15):
    if limiter.allow_request():
        print(f"Request {i+1}: Queued (queue size: {limiter.get_queue_size()})")
    else:
        print(f"Request {i+1}: Rejected (bucket full)")

# Wait and check queue drain
time.sleep(2)
print(f"\nAfter 2 seconds, queue size: {limiter.get_queue_size()}")
```

**Visual Representation:**

```
Leaky Bucket Over Time:

Capacity: 5
Leak Rate: 1 request/second

Time  Queue  Event
â”€â”€â”€â”€  â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0:00  [R1]   Request 1 arrives
0:01  [R2]   R1 leaks, R2 arrives
0:02  [R2,R3,R4] R2 stays, R3,R4 arrive
0:03  [R3,R4,R5] R2 leaks, R5 arrives
0:04  [R4,R5]    R3 leaks
0:05  [R5,R6,R7,R8,R9] Burst arrives
0:06  [R6,R7,R8,R9,R10] R5 leaks, R10 arrives
                        (R11 would be rejected - full!)
0:07  [R7,R8,R9,R10]    R6 leaks
0:08  [R8,R9,R10]       R7 leaks

Queue Visualization:
Size
  5 â”‚        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  4 â”‚      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  3 â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  2 â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  1 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  0 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    0  2  4  6  8  (seconds)
```

**Leaky vs Token Bucket Comparison:**

```
Scenario: 10 requests arrive, then 10 more after 1 second
Limit: 5 requests/second capacity

Token Bucket (capacity=10, refill=5/sec):
â”œâ”€ Time 0: 10 requests
â”‚  â””â”€ 10 tokens available â†’ all allowed immediately âœ“
â”œâ”€ Time 1: 10 requests  
â”‚  â””â”€ 5 tokens refilled â†’ 5 allowed, 5 rejected
â””â”€ Result: Allows bursts (all 10 at once)

Leaky Bucket (capacity=10, leak=5/sec):
â”œâ”€ Time 0: 10 requests
â”‚  â””â”€ All queued (within capacity)
â”‚  â””â”€ Processing at 5/sec â†’ takes 2 seconds to finish
â”œâ”€ Time 1: 10 requests
â”‚  â””â”€ Queue has 5 remaining from previous
â”‚  â””â”€ Can queue 5 more (5+5=10 capacity reached)
â”‚  â””â”€ Remaining 5 rejected âœ—
â””â”€ Result: Smooths traffic (constant outflow rate)
```

**Use Cases:**

```
Leaky Bucket is ideal for:

1. Traffic Smoothing:
   â””â”€ Protect downstream services from bursts
   â””â”€ Example: Message queue processing

2. Network Traffic Shaping:
   â””â”€ Ensure constant bandwidth usage
   â””â”€ Example: Video streaming

3. Background Job Processing:
   â””â”€ Process jobs at predictable rate
   â””â”€ Example: Email sending service

Real Example - Background Worker:
â”œâ”€ Jobs arrive in bursts (1000 at once)
â”œâ”€ Worker can handle 10 jobs/second
â”œâ”€ Leaky bucket queues jobs
â””â”€ Worker processes at steady 10/sec rate
```

**Pros:**

```
âœ“ Smooths out traffic bursts
âœ“ Constant processing rate (predictable)
âœ“ Protects downstream services
âœ“ Simple concept (queue + drain)
```

**Cons:**

```
âœ— No bursts allowed (even if capacity available)
âœ— Queue can introduce latency
âœ— More complex than token bucket
âœ— Older requests can get stale in queue
```

**When to Use:**

```
âœ“ Need constant output rate
âœ“ Protecting downstream services from spikes
âœ“ Background job processing
âœ— Don't use when bursts are acceptable
âœ— Don't use for interactive APIs (latency)
```

-----

### 4.6 Algorithm Comparison Summary

|Algorithm                 |Memory     |Accuracy   |Bursts|Smoothing|Complexity|Use Case                |
|--------------------------|-----------|-----------|------|---------|----------|------------------------|
|**Fixed Window**          |O(1)       |Low        |No    |No       |Simple    |Internal services       |
|**Sliding Window Log**    |O(limit)   |High       |No    |Yes      |Medium    |Security-critical       |
|**Sliding Window Counter**|O(1)       |Medium-High|No    |Yes      |Medium    |Production (most common)|
|**Token Bucket**          |O(1)       |Medium     |Yes   |Yes      |Medium    |API rate limiting       |
|**Leaky Bucket**          |O(capacity)|High       |No    |Yes      |Medium    |Traffic shaping         |

**Decision Tree:**

```
Do you need to allow bursts?
â”œâ”€ YES â†’ Token Bucket
â”‚   â””â”€ Allows controlled bursts while limiting sustained rate
â”‚
â””â”€ NO â†’ Need smooth traffic?
    â”œâ”€ YES â†’ Need constant output?
    â”‚   â”œâ”€ YES â†’ Leaky Bucket (queue-based processing)
    â”‚   â””â”€ NO â†’ Sliding Window Counter (best balance)
    â”‚
    â””â”€ NO â†’ Need perfect accuracy?
        â”œâ”€ YES â†’ Sliding Window Log (memory-intensive)
        â””â”€ NO â†’ Fixed Window (simplest)
```

-----

## 5. Distributed Rate Limiting

### 5.1 The Distributed Challenge

**Problem:** Rate limiting across multiple servers

```
Single Server (Easy):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Web Server   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚In-Memory â”‚ â”‚
â”‚ â”‚ Counter  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Works perfectly!
```

```
Multiple Servers (Hard):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Server 1     â”‚  â”‚ Server 2     â”‚  â”‚ Server 3     â”‚
â”‚ Counter: 50  â”‚  â”‚ Counter: 40  â”‚  â”‚ Counter: 30  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Problem:
â”œâ”€ User limit: 100 requests/minute
â”œâ”€ User hits Server 1: 50 requests
â”œâ”€ User hits Server 2: 40 requests  
â”œâ”€ User hits Server 3: 30 requests
â””â”€ Total: 120 requests (exceeds limit by 20!) âœ—
```

**Why This Happens:**

```
Load Balancer distributes requests:
User makes 120 requests
    â†“
Load Balancer (Round Robin)
    â†“
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“         â†“         â†“         â†“
Server 1  Server 2  Server 3  (repeat)
Count=40  Count=40  Count=40

Each server thinks: "40 < 100, so allow" âœ“
But total across all servers: 120 > 100 âœ—
```

### 5.2 Solution: Centralized Rate Limit Store

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Server 1   â”‚ Server 2   â”‚ Server 3   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚            â”‚            â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Redis Cluster   â”‚
        â”‚  (Shared State)  â”‚
        â”‚                  â”‚
        â”‚ user:123 â†’ 75    â”‚
        â”‚ user:456 â†’ 12    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

All servers share the same counter!
```

**Redis-based Implementation:**

```python
import redis
import time

class DistributedRateLimiter:
    def __init__(self, redis_client, max_requests, window_seconds):
        self.redis = redis_client
        self.max_requests = max_requests
        self.window = window_seconds
    
    def allow_request_fixed_window(self, user_id):
        """Fixed window using Redis"""
        current_window = int(time.time() / self.window)
        key = f"rate_limit:{user_id}:{current_window}"
        
        # Increment counter
        count = self.redis.incr(key)
        
        # Set expiration on first request (TTL = window size)
        if count == 1:
            self.redis.expire(key, self.window)
        
        return count <= self.max_requests
    
    def allow_request_sliding_window(self, user_id):
        """Sliding window log using Redis sorted set"""
        now = time.time()
        key = f"rate_limit:sliding:{user_id}"
        
        # Remove old entries (outside window)
        cutoff = now - self.window
        self.redis.zremrangebyscore(key, 0, cutoff)
        
        # Count current requests
        count = self.redis.zcard(key)
        
        if count < self.max_requests:
            # Add current request with timestamp as score
            self.redis.zadd(key, {str(now): now})
            # Set expiration
            self.redis.expire(key, self.window)
            return True
        
        return False
    
    def allow_request_token_bucket(self, user_id):
        """Token bucket using Redis hash"""
        key = f"rate_limit:token:{user_id}"
        now = time.time()
        
        # Get current bucket state
        pipe = self.redis.pipeline()
        pipe.hget(key, 'tokens')
        pipe.hget(key, 'last_refill')
        tokens, last_refill = pipe.execute()
        
        # Initialize if doesn't exist
        if tokens is None:
            tokens = self.max_requests
            last_refill = now
        else:
            tokens = float(tokens)
            last_refill = float(last_refill)
        
        # Calculate tokens to add
        refill_rate = self.max_requests / self.window
        time_elapsed = now - last_refill
        tokens_to_add = time_elapsed * refill_rate
        tokens = min(self.max_requests, tokens + tokens_to_add)
        
        # Check if request allowed
        if tokens >= 1:
            tokens -= 1
            
            # Update Redis
            pipe = self.redis.pipeline()
            pipe.hset(key, 'tokens', tokens)
            pipe.hset(key, 'last_refill', now)
            pipe.expire(key, self.window * 2)  # Prevent memory leak
            pipe.execute()
            
            return True
        
        return False

# Usage
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
limiter = DistributedRateLimiter(
    redis_client=redis_client,
    max_requests=100,
    window_seconds=60
)

# Check if request allowed
user_id = "user_123"
if limiter.allow_request_fixed_window(user_id):
    print("Request allowed")
else:
    print("Rate limit exceeded")
```

### 5.3 Race Conditions in Distributed Systems

**The Problem:**

```
Redis Operation: INCR (not atomic with check)

Server 1:                    Server 2:
1. GET count = 99            
                             2. GET count = 99
3. Check: 99 < 100 âœ“         
                             4. Check: 99 < 100 âœ“
5. INCR â†’ count = 100        
                             6. INCR â†’ count = 101

Result: 101 requests (exceeded limit!)
```

**Solution: Lua Scripts (Atomic Operations)**

```lua
-- Redis Lua script for atomic rate limiting
-- Script stored in Redis, executed atomically

local key = KEYS[1]
local limit = tonumber(ARGV[1])
local window = tonumber(ARGV[2])
local current_time = tonumber(ARGV[3])

-- Get current count
local current = redis.call('GET', key)

if current == false then
    -- First request - initialize
    redis.call('SET', key, 1, 'EX', window)
    return 1  -- Allowed
elseif tonumber(current) < limit then
    -- Increment count
    redis.call('INCR', key)
    return 1  -- Allowed
else
    -- Limit exceeded
    return 0  -- Rejected
end
```

**Using Lua Script from Python:**

```python
class AtomicDistributedRateLimiter:
    def __init__(self, redis_client, max_requests, window_seconds):
        self.redis = redis_client
        self.max_requests = max_requests
        self.window = window_seconds
        
        # Load Lua script
        self.script = self.redis.register_script("""
            local key = KEYS[1]
            local limit = tonumber(ARGV[1])
            local window = tonumber(ARGV[2])
            
            local current = redis.call('GET', key)
            
            if current == false then
                redis.call('SET', key, 1, 'EX', window)
                return 1
            elseif tonumber(current) < limit then
                redis.call('INCR', key)
                return 1
            else
                return 0
            end
        """)
    
    def allow_request(self, user_id):
        current_window = int(time.time() / self.window)
        key = f"rate_limit:{user_id}:{current_window}"
        
        # Execute Lua script atomically
        result = self.script(
            keys=[key],
            args=[self.max_requests, self.window]
        )
        
        return result == 1
```

**Why Lua Scripts Solve Race Conditions:**

```
Without Lua (Multiple Commands):
Server 1: GET â†’ 99
Server 2: GET â†’ 99
Server 1: INCR â†’ 100
Server 2: INCR â†’ 101 âœ—

With Lua (Single Atomic Operation):
Server 1: Execute Lua script â†’ Check + Increment â†’ 100
Server 2: Execute Lua script â†’ Check + Increment â†’ 101 (rejected) âœ“

Lua script runs atomically in Redis
â””â”€ No other commands can interleave
â””â”€ Guarantees correctness!
```

### 5.4 Synchronization Strategies

**Strategy 1: Strict Synchronization (Redis)**

```
Every request checks Redis:
â”œâ”€ Pro: Accurate across all servers
â”œâ”€ Con: Network latency on every request
â””â”€ Latency: +1-2ms per request

Use when: Accuracy is critical
```

**Strategy 2: Eventual Consistency (Periodic Sync)**

```
Each server maintains local counter:
â”œâ”€ Check local counter (fast)
â”œâ”€ Periodically sync to Redis (every 1 second)
â”œâ”€ Pro: Very fast local checks
â”œâ”€ Con: Temporary inaccuracy
â””â”€ Inaccuracy: Up to 2x limit for 1 second

Implementation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Server 1     â”‚
â”‚ Local: 45    â”‚ â”€â”
â”‚              â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                  â”œâ”€> Every 1 sec
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   sync to Redis
â”‚ Server 2     â”‚  â”‚
â”‚ Local: 38    â”‚ â”€â”¤
â”‚              â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ Server 3     â”‚  â”‚
â”‚ Local: 42    â”‚ â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Redis     â”‚
â”‚ Total: 125   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Strategy 3: Hybrid Approach**

```python
class HybridRateLimiter:
    def __init__(self, redis_client, max_requests, window):
        self.redis = redis_client
        self.max_requests = max_requests
        self.window = window
        self.local_cache = {}
        self.sync_interval = 1  # seconds
        self.last_sync = time.time()
    
    def allow_request(self, user_id):
        now = time.time()
        
        # Sync with Redis periodically
        if now - self.last_sync > self.sync_interval:
            self._sync_to_redis()
            self.last_sync = now
        
        # Check local cache first (fast path)
        if user_id in self.local_cache:
            local_count = self.local_cache[user_id]
            
            # If obviously over limit locally, reject immediately
            if local_count >= self.max_requests:
                return False
            
            # If close to limit, check Redis (accurate)
            if local_count >= self.max_requests * 0.8:
                return self._check_redis(user_id)
        
        # Normal path: increment local counter
        self.local_cache[user_id] = self.local_cache.get(user_id, 0) + 1
        return True
    
    def _check_redis(self, user_id):
        """Accurate check against Redis"""
        # Implementation similar to previous examples
        pass
    
    def _sync_to_redis(self):
        """Sync local counts to Redis"""
        for user_id, count in self.local_cache.items():
            key = f"rate_limit:{user_id}"
            self.redis.incrby(key, count)
        
        # Clear local cache after sync
        self.local_cache.clear()
```

-----

## 6. System Design & Architecture

### 6.1 Where to Place the Rate Limiter?

**Option 1: Application Server (Embedded)**

```
Request â†’ API Gateway â†’ [Rate Limiter] Application Server â†’ Database
                              â†‘
                        Inside app code

Pros:
âœ“ Full context available (user info, request details)
âœ“ Can customize per endpoint
âœ“ No extra network hop

Cons:
âœ— Rate limiting logic in every service
âœ— Code duplication
âœ— Hard to update globally
```

**Option 2: API Gateway / Load Balancer (Middleware)**

```
Request â†’ [Rate Limiter] API Gateway â†’ Application Server â†’ Database
               â†‘
          Before app code

Pros:
âœ“ Centralized logic
âœ“ Protects all backend services
âœ“ Easy to update rules
âœ“ Can reject before reaching app

Cons:
âœ— Limited context (no user details yet)
âœ— Single point of failure
```

**Option 3: Separate Rate Limiting Service**

```
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Request â†’ Gateway â”‚ Rate Limiter    â”‚ â†’ Application Server
               â†“  â”‚  (Side call)    â”‚
               â””â”€â”€â”‚ Redis Cluster   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pros:
âœ“ Highly scalable
âœ“ Reusable across services
âœ“ Independent deployment

Cons:
âœ— Extra network latency
âœ— More complex architecture
```

**Best Practice: Layered Approach**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: API Gateway                        â”‚
â”‚ - Coarse rate limiting (1000 req/sec)      â”‚
â”‚ - Protects from DDoS                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2: Application Service               â”‚
â”‚ - Fine-grained limits (100 req/min)        â”‚
â”‚ - Per-endpoint limits                       â”‚
â”‚ - Per-user tier limits                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Complete Architecture Diagram

```
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  Client   â”‚
                      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  CDN / DDoS    â”‚
                   â”‚  Protection    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Load Balancer / API Gateway          â”‚
â”‚                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Rate Limiter Layer 1                       â”‚ â”‚
â”‚  â”‚  - Global limits (10K QPS)                  â”‚ â”‚
â”‚  â”‚  - IP-based throttling                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           â”‚           â”‚
        â–¼           â–¼           â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ App    â”‚  â”‚ App    â”‚  â”‚ App    â”‚
   â”‚Server 1â”‚  â”‚Server 2â”‚  â”‚Server 3â”‚
   â”‚        â”‚  â”‚        â”‚  â”‚        â”‚
   â”‚â”Œâ”€â”€â”€â”€â”€â”€â”â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”â”‚
   â”‚â”‚Rate  â”‚â”‚  â”‚â”‚Rate  â”‚â”‚  â”‚â”‚Rate  â”‚â”‚
   â”‚â”‚Limit â”‚â”‚  â”‚â”‚Limit â”‚â”‚  â”‚â”‚Limit â”‚â”‚
   â”‚â”‚Layer2â”‚â”‚  â”‚â”‚Layer2â”‚â”‚  â”‚â”‚Layer2â”‚â”‚
   â”‚â””â”€â”€â”€â”¬â”€â”€â”˜â”‚  â”‚â””â”€â”€â”€â”¬â”€â”€â”˜â”‚  â”‚â””â”€â”€â”€â”¬â”€â”€â”˜â”‚
   â””â”€â”€â”€â”€â”¼â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¼â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¼â”€â”€â”€â”˜
        â”‚           â”‚           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Redis Cluster       â”‚
        â”‚  (Rate Limit State)   â”‚
        â”‚                       â”‚
        â”‚  Node 1   Node 2      â”‚
        â”‚  Master   Replica     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Application Database â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 Rate Limiting Dimensions

**What to Rate Limit On?**

```
1. Per User ID:
   â””â”€ Limit: 100 requests/minute per authenticated user
   â””â”€ Use: Prevent individual user abuse

2. Per IP Address:
   â””â”€ Limit: 1000 requests/hour per IP
   â””â”€ Use: DDoS protection, anonymous users

3. Per API Key:
   â””â”€ Limit: 10,000 requests/day per API key
   â””â”€ Use: Third-party API integrations

4. Per Endpoint:
   â””â”€ Expensive endpoint: 10 req/min
   â””â”€ Cheap endpoint: 1000 req/min
   â””â”€ Use: Protect resource-intensive operations

5. Global:
   â””â”€ Limit: 100K QPS total across all users
   â””â”€ Use: Protect server capacity

6. Combination (Multi-dimensional):
   â””â”€ User X on Endpoint Y: 10 req/min
   â””â”€ Same user on other endpoints: 100 req/min
```

**Implementation Example:**

```python
class MultiDimensionalRateLimiter:
    def __init__(self, redis_client):
        self.redis = redis_client
        
        # Define limits for different dimensions
        self.limits = {
            'user': {'max': 100, 'window': 60},      # 100/min per user
            'ip': {'max': 1000, 'window': 3600},     # 1000/hour per IP
            'api_key': {'max': 10000, 'window': 86400}, # 10K/day per key
            'endpoint:/expensive': {'max': 10, 'window': 60},
            'endpoint:/cheap': {'max': 1000, 'window': 60},
        }
    
    def allow_request(self, request_context):
        """
        request_context = {
            'user_id': 'user_123',
            'ip': '192.168.1.1',
            'api_key': 'key_abc',
            'endpoint': '/expensive'
        }
        """
        # Check all applicable dimensions
        checks = []
        
        # User-based limit
        if 'user_id' in request_context:
            checks.append(('user', request_context['user_id']))
        
        # IP-based limit
        if 'ip' in request_context:
            checks.append(('ip', request_context['ip']))
        
        # API key limit
        if 'api_key' in request_context:
            checks.append(('api_key', request_context['api_key']))
        
        # Endpoint limit
        if 'endpoint' in request_context:
            endpoint_key = f"endpoint:{request_context['endpoint']}"
            if endpoint_key in self.limits:
                checks.append((endpoint_key, request_context['endpoint']))
        
        # Check each dimension
        for dimension, identifier in checks:
            if not self._check_limit(dimension, identifier):
                return False, f"Rate limit exceeded for {dimension}"
        
        return True, "Allowed"
    
    def _check_limit(self, dimension, identifier):
        """Check single dimension limit"""
        if dimension not in self.limits:
            return True
        
        config = self.limits[dimension]
        key = f"rate_limit:{dimension}:{identifier}"
        
        # Use fixed window for simplicity
        current_window = int(time.time() / config['window'])
        redis_key = f"{key}:{current_window}"
        
        count = self.redis.incr(redis_key)
        if count == 1:
            self.redis.expire(redis_key, config['window'])
        
        return count <= config['max']
```

### 6.4 Response Headers & User Experience

**Standard HTTP Headers:**

```http
HTTP/1.1 429 Too Many Requests
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 0
X-RateLimit-Reset: 1640000000
Retry-After: 60

{
  "error": "Rate limit exceeded",
  "message": "You have exceeded 100 requests per minute",
  "retry_after_seconds": 60
}
```

**Header Meanings:**

```
X-RateLimit-Limit: 100
â””â”€ Maximum requests allowed in window

X-RateLimit-Remaining: 0
â””â”€ How many requests left in current window

X-RateLimit-Reset: 1640000000
â””â”€ Unix timestamp when limit resets

Retry-After: 60
â””â”€ Seconds until user can retry
```

**Implementation:**

```python
from flask import Flask, jsonify, request
import time

app = Flask(__name__)
limiter = DistributedRateLimiter(...)

@app.before_request
def rate_limit_check():
    user_id = request.headers.get('X-User-ID', request.remote_addr)
    
    # Get current limit status
    allowed, remaining, reset_time = limiter.check_limit(user_id)
    
    # Add headers to response
    @app.after_request
    def add_rate_limit_headers(response):
        response.headers['X-RateLimit-Limit'] = '100'
        response.headers['X-RateLimit-Remaining'] = str(remaining)
        response.headers['X-RateLimit-Reset'] = str(reset_time)
        return response
    
    if not allowed:
        retry_after = reset_time - int(time.time())
        return jsonify({
            'error': 'Rate limit exceeded',
            'retry_after_seconds': retry_after
        }), 429, {
            'Retry-After': str(retry_after),
            'X-RateLimit-Limit': '100',
            'X-RateLimit-Remaining': '0',
            'X-RateLimit-Reset': str(reset_time)
        }

@app.route('/api/data')
def get_data():
    return jsonify({'data': 'your data here'})
```

-----

## 7. Advanced Topics

### 7.1 Tiered Rate Limiting

**Different Limits for Different User Tiers:**

```
User Tiers:
â”œâ”€ Free: 100 requests/hour
â”œâ”€ Basic: 1,000 requests/hour
â”œâ”€ Pro: 10,000 requests/hour
â””â”€ Enterprise: 100,000 requests/hour

Implementation:
```

```python
class TieredRateLimiter:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.tier_limits = {
            'free': {'max': 100, 'window': 3600},
            'basic': {'max': 1000, 'window': 3600},
            'pro': {'max': 10000, 'window': 3600},
            'enterprise': {'max': 100000, 'window': 3600},
        }
    
    def get_user_tier(self, user_id):
        """Fetch user's subscription tier from database"""
        # In production: query database or cache
        # For example: return db.query("SELECT tier FROM users WHERE id = ?", user_id)
        return 'free'  # Placeholder
    
    def allow_request(self, user_id):
        tier = self.get_user_tier(user_id)
        limit_config = self.tier_limits.get(tier, self.tier_limits['free'])
        
        # Apply rate limit based on tier
        key = f"rate_limit:{tier}:{user_id}"
        current_window = int(time.time() / limit_config['window'])
        redis_key = f"{key}:{current_window}"
        
        count = self.redis.incr(redis_key)
        if count == 1:
            self.redis.expire(redis_key, limit_config['window'])
        
        return count <= limit_config['max']
```

### 7.2 Adaptive Rate Limiting

**Dynamically Adjust Limits Based on System Load:**

```python
class AdaptiveRateLimiter:
    def __init__(self, redis_client, base_limit=1000):
        self.redis = redis_client
        self.base_limit = base_limit
    
    def get_system_load(self):
        """Monitor system metrics"""
        # In production: query monitoring system (Prometheus, CloudWatch)
        cpu_usage = 0.75  # 75% CPU
        memory_usage = 0.60  # 60% memory
        queue_depth = 100  # 100 pending requests
        
        return {
            'cpu': cpu_usage,
            'memory': memory_usage,
            'queue': queue_depth
        }
    
    def calculate_dynamic_limit(self):
        """Adjust limit based on system health"""
        metrics = self.get_system_load()
        
        # If system overloaded, reduce limits
        if metrics['cpu'] > 0.8 or metrics['memory'] > 0.8:
            multiplier = 0.5  # Reduce to 50%
        elif metrics['cpu'] > 0.6 or metrics['memory'] > 0.6:
            multiplier = 0.75  # Reduce to 75%
        else:
            multiplier = 1.0  # Normal limits
        
        return int(self.base_limit * multiplier)
    
    def allow_request(self, user_id):
        current_limit = self.calculate_dynamic_limit()
        
        # Use current limit for rate limiting
        # ... (similar to previous implementations)
```

**Visualization:**

```
System Load vs Rate Limit:

Limit
1000 â”‚          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
     â”‚      â–ˆâ–ˆâ–ˆâ–ˆ
 750 â”‚  â–ˆâ–ˆâ–ˆâ–ˆ
     â”‚â–ˆâ–ˆ
 500 â”‚
     â”‚
   0 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     0   20   40   60   80  100 (% CPU)

When CPU usage increases:
â””â”€ Automatically reduce rate limits
â””â”€ Protects system from overload
â””â”€ Gradually increase when load decreases
```

### 7.3 Rate Limiting in Microservices

**Challenge: Each Service Needs Rate Limiting**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service  â”‚â”€â”€â”€â–¶â”‚ Service  â”‚â”€â”€â”€â–¶â”‚ Service  â”‚
â”‚    A     â”‚    â”‚    B     â”‚    â”‚    C     â”‚
â”‚(Gateway) â”‚    â”‚ (Core)   â”‚    â”‚  (DB)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚    Redis    â”‚
              â”‚ (Shared)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Problem:
â”œâ”€ Service A â†’ Service B: Need rate limit
â”œâ”€ Service B â†’ Service C: Need rate limit
â””â”€ User â†’ Service A: Need rate limit

Solution: Hierarchical Rate Limiting
```

**Implementation:**

```python
class MicroserviceRateLimiter:
    def __init__(self, redis_client, service_name):
        self.redis = redis_client
        self.service_name = service_name
        
        # Define limits per service pair
        self.service_limits = {
            'gateway->auth': {'max': 1000, 'window': 60},
            'gateway->core': {'max': 500, 'window': 60},
            'core->database': {'max': 100, 'window': 60},
        }
    
    def allow_request(self, from_service, to_service, identifier):
        """
        Check rate limit for inter-service call
        
        from_service: Calling service
        to_service: Target service
        identifier: Request identifier (user_id, request_id, etc.)
        """
        service_pair = f"{from_service}->{to_service}"
        
        if service_pair not in self.service_limits:
            return True  # No limit defined
        
        limit_config = self.service_limits[service_pair]
        key = f"rate_limit:service:{service_pair}:{identifier}"
        
        # Apply rate limit
        current_window = int(time.time() / limit_config['window'])
        redis_key = f"{key}:{current_window}"
        
        count = self.redis.incr(redis_key)
        if count == 1:
            self.redis.expire(redis_key, limit_config['window'])
        
        return count <= limit_config['max']
```

### 7.4 Rate Limiting for DDoS Protection

**Multi-Layer Defense:**

```
Layer 1: Infrastructure (Cloudflare, AWS Shield)
â”œâ”€ Filter malicious traffic
â”œâ”€ Block known bad IPs
â””â”€ Rate limit: 100K QPS per IP

Layer 2: API Gateway
â”œâ”€ Geographic rate limiting
â”œâ”€ Rate limit: 10K QPS per region
â””â”€ Detect anomalous patterns

Layer 3: Application
â”œâ”€ User-based rate limiting
â”œâ”€ Endpoint-specific limits
â””â”€ Behavior analysis

Layer 4: Database
â”œâ”€ Query rate limiting
â”œâ”€ Connection pooling
â””â”€ Read replica distribution
```

**Anomaly Detection:**

```python
class AnomalyDetectionRateLimiter:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.baseline_window = 3600  # 1 hour
    
    def detect_anomaly(self, user_id):
        """Detect if user behavior is anomalous"""
        # Get historical average
        historical_key = f"baseline:{user_id}"
        baseline = self.redis.get(historical_key)
        
        if baseline is None:
            baseline = 10  # Default
        else:
            baseline = float(baseline)
        
        # Get current rate
        current_key = f"current:{user_id}"
        current_rate = self.redis.get(current_key) or 0
        current_rate = float(current_rate)
        
        # If current rate >> baseline, it's anomalous
        if current_rate > baseline * 10:  # 10x normal rate
            return True, "Anomalous traffic detected"
        
        return False, "Normal"
    
    def update_baseline(self, user_id, current_rate):
        """Update user's baseline (rolling average)"""
        key = f"baseline:{user_id}"
        
        # Exponential moving average
        alpha = 0.1
        old_baseline = self.redis.get(key) or current_rate
        old_baseline = float(old_baseline)
        
        new_baseline = alpha * current_rate + (1 - alpha) * old_baseline
        
        self.redis.set(key, new_baseline, ex=86400)  # 24-hour TTL
```

-----

## 8. Real-World Case Studies

### 8.1 GitHub API Rate Limiting

**Implementation:**

```
Unauthenticated: 60 requests/hour per IP
Authenticated: 5,000 requests/hour per user
Enterprise: 15,000 requests/hour

Algorithm: Token Bucket (allows bursts)

Headers:
X-RateLimit-Limit: 5000
X-RateLimit-Remaining: 4999
X-RateLimit-Reset: 1640000000
X-RateLimit-Used: 1
X-RateLimit-Resource: core
```

**Why Token Bucket:**

```
GitHub allows developers to:
â”œâ”€ Make occasional bursts (clone large repos)
â”œâ”€ Sustained usage limited to 5K/hour
â””â”€ Token bucket perfect for this pattern
```

### 8.2 Stripe API Rate Limiting

**Implementation:**

```
Rate limits:
â”œâ”€ Standard: 100 requests/second
â”œâ”€ Bursts allowed up to 1,000 requests
â”œâ”€ Uses Token Bucket algorithm

Special handling:
â”œâ”€ Higher limits for trusted partners
â”œâ”€ Gradual backoff for violations
â””â”€ Whitelist for critical integrations
```

**Response:**

```http
HTTP/1.1 429 Too Many Requests
Stripe-RateLimit-Limit: 100
Stripe-RateLimit-Remaining: 0
Stripe-RateLimit-Reset: 1640000000

{
  "error": {
    "type": "rate_limit_error",
    "message": "Too many requests. Please slow down."
  }
}
```

### 8.3 Twitter API Rate Limiting

**Implementation:**

```
Different endpoints, different limits:
â”œâ”€ GET tweets/search: 180 requests / 15 min
â”œâ”€ POST tweets/create: 300 requests / 3 hours
â”œâ”€ GET users/lookup: 900 requests / 15 min

Window: 15 minutes (sliding window)
Algorithm: Sliding Window Counter

Per-endpoint tracking:
â””â”€ Each endpoint has separate counter
```

**Why Different Limits:**

```
Read operations (GET): Higher limits
â”œâ”€ Less resource-intensive
â”œâ”€ Cached responses
â””â”€ Example: 900 req / 15 min

Write operations (POST): Lower limits
â”œâ”€ More expensive (database writes)
â”œâ”€ Spam prevention
â””â”€ Example: 300 req / 3 hours

This protects backend while allowing flexibility
```

-----

## 9. Interview Framework

### 9.1 How to Approach â€œDesign a Rate Limiterâ€

**Step 1: Requirements Clarification (3-5 min)**

```
Questions to Ask:

Functional:
â”œâ”€ What type of rate limiter? (User, IP, API key?)
â”œâ”€ What's the rate? (100/min, 1000/hour, etc.)
â”œâ”€ Distributed or single server?
â”œâ”€ Hard block or throttle? (reject vs delay)

Non-Functional:
â”œâ”€ Scale: How many users? Requests?
â”œâ”€ Latency: What overhead is acceptable?
â”œâ”€ Accuracy: Strict or approximate?
â””â”€ Fault tolerance: Fail open or closed?

Example Answer:
"Let me clarify:
- We're rate limiting API requests per user
- Limit: 100 requests per minute
- Distributed system (multiple servers)
- Hard block (return 429 error)
- Scale: 10M users, 100K QPS
- Latency: < 1ms overhead
- Accuracy: 99%+ (some edge case tolerance OK)
Is this correct?"
```

**Step 2: Choose Algorithm (2-3 min)**

```
Decision Framework:

Need bursts?
â”œâ”€ YES â†’ Token Bucket
â””â”€ NO â†’ Need perfect accuracy?
    â”œâ”€ YES â†’ Sliding Window Log
    â””â”€ NO â†’ Sliding Window Counter (recommended!)

Explain your choice:
"I'll use Sliding Window Counter because:
âœ“ Memory efficient: O(1) per user
âœ“ Good accuracy (no 2x boundary issue)
âœ“ Fast: O(1) operations
âœ“ Production-proven (used by Twitter, Stripe)

Trade-off: Slight estimation vs perfect accuracy
â””â”€ But 99%+ accuracy is acceptable for this use case"
```

**Step 3: High-Level Design (5 min)**

```
Draw Architecture:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Gateway / Load Balancer         â”‚
â”‚ - Rate limit check (Layer 1)       â”‚
â”‚ - IP-based: 1000 req/sec           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚
        â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ App Server 1â”‚ â”‚ App Server 2â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Redis Cluster (Rate Limit State) â”‚
â”‚ - Sliding window counters        â”‚
â”‚ - user:123 â†’ {prev: 80, curr: 30}â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Request Flow:
1. Request â†’ Load Balancer
2. Check Redis for user's rate limit
3. If allowed â†’ forward to app server
4. If rejected â†’ return 429
```

**Step 4: Deep Dive (15-20 min)**

**A) Algorithm Implementation:**

```
Explain Sliding Window Counter:

"At time 01:15 (15 seconds into window 2):

Window 1 [00:00-01:00]: 80 requests
Window 2 [01:00-02:00]: 30 requests (so far)

Calculate:
â”œâ”€ Time in current window: 15 seconds (25%)
â”œâ”€ Weight previous window: 1 - 0.25 = 0.75
â”œâ”€ Estimate: (80 Ã— 0.75) + 30 = 60 + 30 = 90
â””â”€ 90 < 100 â†’ Allow âœ“

Redis data structure:
{
  'user:123:window:1': 80,
  'user:123:window:2': 30
}

On new request:
1. Incr current window counter
2. Check weighted sum
3. Return allow/deny
"
```

**B) Distributed System Handling:**

```
"Challenge: Multiple servers accessing Redis

Race condition:
Server 1: READ count=99 â†’ INCR â†’ count=100 âœ“
Server 2: READ count=99 â†’ INCR â†’ count=101 âœ—

Solution: Lua Script (atomic)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
local count = redis.call('GET', key)
if count < limit then
  redis.call('INCR', key)
  return 1  -- allowed
else
  return 0  -- rejected
end
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

This runs atomically in Redis
â””â”€ Prevents race conditions"
```

**C) Response Headers:**

```
"Return helpful headers:

HTTP/1.1 429 Too Many Requests
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 0
X-RateLimit-Reset: 1640000000
Retry-After: 45

Benefits:
â”œâ”€ User knows limit (100)
â”œâ”€ User knows when to retry (45 sec)
â””â”€ Better developer experience"
```

**Step 5: Optimizations (5 min)**

```
Optimizations to Discuss:

1. Hot User Problem:
   "Celebrity users hit same Redis key
   
   Solution:
   â”œâ”€ Local cache (L1) in app servers
   â”œâ”€ Cache limit state for 1 second
   â””â”€ Reduces Redis load by 10-100x"

2. Redis Failure:
   "What if Redis goes down?
   
   Options:
   â”œâ”€ Fail Open: Allow all requests (availability)
   â”œâ”€ Fail Closed: Reject all (security)
   â””â”€ Hybrid: Use local fallback counters (temporary)"

3. Memory Optimization:
   "Clean up old entries:
   â”œâ”€ Set TTL on Redis keys (auto-expire)
   â”œâ”€ Cleanup job removes inactive users
   â””â”€ Prevents memory leak"
```

**Step 6: Monitoring (2 min)**

```
"Key Metrics:

1. Rejection Rate:
   â””â”€ % of requests rejected
   â””â”€ Alert if suddenly spikes

2. Top Rate-Limited Users:
   â””â”€ Identify potential abusers
   â””â”€ Or legitimate users needing upgrade

3. Rate Limiter Latency:
   â””â”€ p99 latency < 1ms
   â””â”€ Redis response time

4. False Positive Rate:
   â””â”€ Legitimate requests rejected
   â””â”€ Should be < 0.1%"
```

### 9.2 Common Follow-up Questions

**Q1: â€œHow would you handle different rate limits for different user tiers (free vs paid)?â€**

```
Answer:

"Store user tier in cache/database:

user_id â†’ tier (free/pro/enterprise)

Rate limit logic:
â”œâ”€ Fetch user tier
â”œâ”€ Apply tier-specific limit
â”‚  â”œâ”€ Free: 100 req/min
â”‚  â”œâ”€ Pro: 1000 req/min
â”‚  â””â”€ Enterprise: 10000 req/min
â””â”€ Use same algorithm (sliding window)

Redis key includes tier:
rate_limit:{tier}:{user_id}:{window}

This allows easy tier upgrades:
â””â”€ Change tier â†’ new limits apply immediately"
```

**Q2: â€œUser complains theyâ€™re being rate limited unfairly. How do you debug?â€**

```
Answer:

"Debugging Steps:

1. Check Logs:
   â””â”€ Look up user's request history
   â””â”€ Verify actual request count

2. Check Redis State:
   â””â”€ Redis CLI: GET rate_limit:user:123:*
   â””â”€ See current counters

3. Check Clock Skew:
   â””â”€ Are servers in sync?
   â””â”€ Time drift can cause issues

4. Check for Multiple Accounts:
   â””â”€ Same IP, different user IDs?
   â””â”€ Shared API key?

5. Verify Rate Limit Rules:
   â””â”€ Is limit configured correctly?
   â””â”€ Did rules change recently?

If legitimate:
â”œâ”€ Temporarily increase limit
â”œâ”€ Whitelist user
â””â”€ Investigate algorithm accuracy"
```

**Q3: â€œA new feature launches and traffic spikes 10x. What happens to rate limiting?â€**

```
Answer:

"Immediate Impact:
â”œâ”€ More users hit rate limits
â”œâ”€ Redis load increases
â””â”€ Potential system overload

Short-term Response:
â”œâ”€ Monitor rejection rate
â”œâ”€ Consider temporary limit increase
â”œâ”€ Add more Redis nodes if needed
â””â”€ Enable request prioritization

Long-term Solution:
â”œâ”€ Adaptive rate limiting
â”‚  â””â”€ Adjust limits based on system load
â”œâ”€ Graceful degradation
â”‚  â””â”€ Prioritize critical endpoints
â””â”€ Capacity planning
   â””â”€ Scale infrastructure proactively

Trade-off Discussion:
'We could:
A) Keep strict limits (protect system)
B) Temporarily relax (better UX)
C) Tier-based (free users limited, paid OK)

I'd choose C because:
âœ“ Protects system
âœ“ Good UX for paying customers
âœ“ Incentivizes upgrades'"
```

### 9.3 Red Flags to Avoid

**âŒ Donâ€™t Say:**

```
1. "Just use fixed window, it's simple"
   â†³ Shows lack of depth (boundary problem)

2. "Store all timestamps in memory"
   â†³ Doesn't scale (memory explosion)

3. "Rate limit in database"
   â†³ Too slow (defeats purpose)

4. "Don't need distributed state"
   â†³ Ignores multi-server reality

5. "Block attackers permanently"
   â†³ Too harsh (could be legitimate spike)
```

**âœ… Do Say:**

```
1. "I'll use sliding window counter because:
   - Good accuracy without memory overhead
   - Production-proven at scale
   - Trade-off: slight estimation vs perfect accuracy"

2. "For 10M users Ã— 100 requests:
   - Sliding window log: 1M Ã— 100 Ã— 8 bytes = 800MB
   - Sliding window counter: 1M Ã— 2 counters Ã— 8 bytes = 16MB
   - Clear winner: counter approach"

3. "Rate limiter needs < 1ms latency:
   - In-memory cache (Redis) required
   - Database too slow (10-50ms)
   - Network round-trip to Redis: ~1ms acceptable"

4. "Multiple servers require shared state:
   - Redis cluster for coordination
   - Lua scripts for atomic operations
   - Prevents race conditions"

5. "For DDoS protection:
   - Layer 1: Infrastructure (Cloudflare)
   - Layer 2: IP rate limiting (gateway)
   - Layer 3: User rate limiting (app)
   - Layer 4: Graceful degradation (fallback)"
```

-----

## Summary Checklist

Before your interview, ensure you can explain:

**Core Concepts:**

- [ ] Why rate limiting is needed (protection, fairness, cost)
- [ ] The boundary problem in fixed window
- [ ] Why distributed systems need shared state

**Algorithms:**

- [ ] Fixed Window (simple but boundary problem)
- [ ] Sliding Window Log (accurate but memory-heavy)
- [ ] Sliding Window Counter (best balance, production choice)
- [ ] Token Bucket (allows bursts)
- [ ] Leaky Bucket (smooths traffic)

**Implementation:**

- [ ] Redis-based distributed rate limiting
- [ ] Lua scripts for atomic operations
- [ ] Race condition prevention
- [ ] Response headers (X-RateLimit-*)

**System Design:**

- [ ] Where to place rate limiter (gateway vs app)
- [ ] Multi-dimensional rate limiting (user, IP, endpoint)
- [ ] Tiered limits (free vs paid)
- [ ] Failure modes (fail open vs closed)

**Advanced Topics:**

- [ ] DDoS protection strategies
- [ ] Adaptive rate limiting
- [ ] Microservices rate limiting
- [ ] Monitoring and debugging

**Interview Skills:**

- [ ] Requirements clarification questions
- [ ] Algorithm selection justification
- [ ] Drawing clear architecture diagrams
- [ ] Explaining trade-offs
- [ ] Handling follow-up questions

-----

## Next Steps

**Practice:**

1. Implement sliding window counter in your language
1. Draw the architecture from memory
1. Explain token bucket to someone else
1. Walk through distributed race condition scenario

**Additional Topics to Explore:**

- Circuit Breaker pattern (related concept)
- API Gateway implementations (Kong, Nginx)
- Redis clustering and replication
- Prometheus metrics for rate limiting

**Related System Design Problems:**

- API Gateway design
- DDoS protection system
- Load Balancer design
- Throttling and backpressure

-----

**Good luck with your interviews!** Remember: Rate limiting is about trade-offs between accuracy, performance, and user experience. Show your thinking process, explain your choices, and discuss alternatives.